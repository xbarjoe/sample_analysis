{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: The output of this notebook has been cleared to reduce storage space while emailing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db63351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, make_scorer\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "#Config tensorflow\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and display a summary\n",
    "dataset = pd.read_csv(r\"./data/training.csv\", delimiter=\",\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing possible values for categorical features to gain insight on how to proceed with numeric conversions\n",
    "categorical = ['DataSource1_Feature1','DataSource1_Feature2','DataSource1_Feature3','DataSource3_Feature3','DataSource4_Feature6']\n",
    "for feature in categorical:\n",
    "    print(dataset[feature].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbae867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firstly generate feature combination graphs to look for immediately interesting features\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "combinations = itertools.combinations(dataset.columns, 2)\n",
    "\n",
    "for combo in combinations:\n",
    "    #We can immediately Isolate DataSource1_Feature 3 here because it contains the same value for all entries\n",
    "    if 'ID' in combo or 'Target' in combo or 'DataSource1_Feature3' in combo:\n",
    "        continue\n",
    "    sns.scatterplot(x=combo[0], y=combo[1], data=dataset, hue='Target')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2417a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the graphs above, I've selected the following features as being candidates for valueable features.\n",
    "#DataSource1_Feature1\n",
    "#DataSource2_Feature1\n",
    "#DataSource3_Feature2\n",
    "#DataSource3_Feature3\n",
    "\n",
    "#We'll create a separate dataframe containing just these features and the target\n",
    "isolated_features = dataset[['DataSource1_Feature1','DataSource2_Feature1','DataSource3_Feature2','DataSource3_Feature3','DataSource4_Feature6','DataSource4_Feature5','Target']]\n",
    "isolated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see from the dataframe summary above, we have to tend to some missing values\n",
    "isolated_features.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataSource3 seems to be a very sparse datasource, so we can try just removing it for the time being\n",
    "isolated_features = isolated_features[['DataSource1_Feature1','DataSource2_Feature1','DataSource4_Feature6','DataSource4_Feature5','Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b24a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9892fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can go ahead and One-Hot encode the categorical variables\n",
    "isolated_features = pd.get_dummies(isolated_features, columns=['DataSource1_Feature1','DataSource4_Feature6'], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We still have to tend to the missing values in DataSource2_Feature1, we'll use a KNN method to deal with those.\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "isolated_features = pd.DataFrame(imputer.fit_transform(isolated_features),\n",
    "                                 columns = isolated_features.columns)\n",
    "\n",
    "#and make sure everything looks good\n",
    "isolated_features.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll also go ahead and scale / normalize the data while we're at it, using the Min-Max method\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "isolated_features = pd.DataFrame(scaler.fit_transform(isolated_features),\n",
    "                                 columns = isolated_features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde73c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the pandas objects to numpy arrays\n",
    "target = isolated_features.pop('Target')\n",
    "X = isolated_features.to_numpy()\n",
    "Y = target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using K-Folds cross-validation and an 80:20 Train/Validiton split, run a simple logistic regression\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "for train, test in kfold.split(X, Y):\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=18, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['AUC'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(X[train], Y[train], epochs=100, batch_size=64, verbose=1, validation_split=0.2, callbacks=[callback])\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=1)\n",
    "    \n",
    "    #Plot the Training / Validation Loss\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a confusion matrix to get some insight into the performance\n",
    "y_pred = model.predict(X[test])\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "cm = confusion_matrix(y_pred, Y[test])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try adding back and cleaning up the sparse columns and see if that causes any improvement.\n",
    "dataset = pd.read_csv(r\"./data/training.csv\", delimiter=\",\")\n",
    "isolated_features = dataset[['DataSource1_Feature1','DataSource2_Feature1','DataSource3_Feature2','DataSource3_Feature3','DataSource4_Feature6','DataSource4_Feature5','Target']]\n",
    "\n",
    "isolated_features = pd.get_dummies(isolated_features,\n",
    "                                   columns=['DataSource1_Feature1','DataSource4_Feature6','DataSource3_Feature3'],\n",
    "                                   dtype=float)\n",
    "\n",
    "#Using the KNN to replace ALL of the missing data\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "isolated_features = pd.DataFrame(imputer.fit_transform(isolated_features),\n",
    "                                 columns = isolated_features.columns)\n",
    "\n",
    "#Scaling the data again\n",
    "scaler = MinMaxScaler()\n",
    "isolated_features = pd.DataFrame(scaler.fit_transform(isolated_features),\n",
    "                                 columns = isolated_features.columns)\n",
    "\n",
    "#re-defining our training variables\n",
    "target = isolated_features.pop('Target')\n",
    "X = isolated_features.to_numpy()\n",
    "Y = target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4418ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using K-Folds cross-validation and an 80:20 Train/Validiton split, re-run the logisitc regression with the new data\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=25, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['AUC'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(X[train],\n",
    "                        Y[train],\n",
    "                        epochs=100,\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[callback])\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=1)\n",
    "    \n",
    "    #Save the model to the disk\n",
    "    model.save(\"./model.h5\")\n",
    "    \n",
    "    #Plot the Training / Validation Loss\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d84e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a new confusion matrix\n",
    "y_pred = model.predict(X[test])\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "cm = confusion_matrix(y_pred, Y[test])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Marginally better, perhaps adding back additional features will further increase our performance\n",
    "dataset = pd.read_csv(r\"./data/training.csv\", delimiter=\",\")\n",
    "isolated_features = dataset[['DataSource1_Feature1','DataSource2_Feature1','DataSource3_Feature2','DataSource3_Feature3','DataSource4_Feature6','DataSource4_Feature5','DataSource3_Feature1','DataSource2_Feature9','Target']]\n",
    "\n",
    "isolated_features = pd.get_dummies(isolated_features,\n",
    "                                   columns=['DataSource1_Feature1','DataSource4_Feature6','DataSource3_Feature3'],\n",
    "                                   dtype=float)\n",
    "\n",
    "#Using the KNN to replace ALL of the missing data\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "isolated_features = pd.DataFrame(imputer.fit_transform(isolated_features),\n",
    "                                 columns = isolated_features.columns)\n",
    "\n",
    "#Scaling the data again\n",
    "scaler = MinMaxScaler()\n",
    "isolated_features = pd.DataFrame(scaler.fit_transform(isolated_features),\n",
    "                                 columns = isolated_features.columns)\n",
    "\n",
    "#re-defining our training variables\n",
    "target = isolated_features.pop('Target')\n",
    "X = isolated_features.to_numpy()\n",
    "Y = target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a685b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using K-Folds cross-validation and an 80:20 Train/Validiton split, re-run the logisitc regression with the new data\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=27, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['AUC'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(X[train],\n",
    "                        Y[train],\n",
    "                        epochs=100,\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[callback])\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=1)\n",
    "    \n",
    "    #Plot the Training / Validation Loss\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a new confusion matrix\n",
    "y_pred = model.predict(X[test])\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "cm = confusion_matrix(y_pred, Y[test])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It appears to preform worse now, perhaps a different model approach will put us in the right direction\n",
    "#We'll re-use the setup from the second logistic regression, as that seemed to be the highest performer.\n",
    "\n",
    "dataset = pd.read_csv(r\"./data/training.csv\", delimiter=\",\")\n",
    "isolated_features = dataset[['DataSource1_Feature1','DataSource2_Feature1','DataSource3_Feature2','DataSource3_Feature3','DataSource4_Feature6','DataSource4_Feature5','Target']]\n",
    "isolated_features = pd.get_dummies(isolated_features,\n",
    "                                   columns=['DataSource1_Feature1','DataSource4_Feature6','DataSource3_Feature3'],\n",
    "                                   dtype=float)\n",
    "\n",
    "#Using the KNN to replace ALL of the missing data\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "isolated_features = pd.DataFrame(imputer.fit_transform(isolated_features),\n",
    "                                 columns = isolated_features.columns)\n",
    "\n",
    "#Scaling the data again\n",
    "scaler = MinMaxScaler()\n",
    "isolated_features = pd.DataFrame(scaler.fit_transform(isolated_features),\n",
    "                                 columns = isolated_features.columns)\n",
    "\n",
    "#re-defining our training variables\n",
    "target = isolated_features.pop('Target')\n",
    "X = isolated_features.to_numpy()\n",
    "Y = target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22110b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying a Random Forest model for our next approach\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20)\n",
    "\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# evaluate the model\n",
    "model.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# report performance\n",
    "print(roc_auc_score(Y_test, y_pred))\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "cm = confusion_matrix(y_pred, Y_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65077e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The random forest model performed about the same as the logistic regression, even after some additional tweaking / tuning\n",
    "#(Omitted for the sake of brevity)\n",
    "#The similar performance regardless of tuning, class weighting, feature engineering, etc could suggest that the positive \n",
    "#classifications might not be distinct enough for patterns to effectively emerge in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe37bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before proceeding with one of our previously detailed models, we'll pull out all the stops and try a neural network\n",
    "#with some additional feature selection techniques.\n",
    "\n",
    "#Reload the data\n",
    "dataset = pd.read_csv(r\"./data/training.csv\", delimiter=\",\")\n",
    "\n",
    "#Remove DataSource1_Feature3 from dataframe due to it containing the same value for all examples\n",
    "#(For the sake of simplicity, I'll some of the dataframe modifications in-place, which isn't always ideal, but will\n",
    "#suffice for our purposes)\n",
    "\n",
    "#Similarly, remove it from our running list of categorical features\n",
    "dataset.drop('DataSource1_Feature3', axis = 1, inplace = True)\n",
    "categorical.pop(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a7fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also drop the \"ID\" column, as it doesn't provide us with any useful information\n",
    "dataset.drop('ID', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c3a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot encode the remaining categorical features (ignoring the NaN values in these for now)\n",
    "numeric_dataset = pd.get_dummies(dataset,\n",
    "                                 columns=categorical,\n",
    "                                 dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1febee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many entries in the dataset contain NaN values\n",
    "null_data = numeric_dataset[numeric_dataset.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9392f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The majority of the entries contain at least 1 NaN value, so it's in our best interest to take a more granular look,\n",
    "#rather than just, say, call something like numeric_dataset.dropna(), which would effectively remove all of the null values\n",
    "\n",
    "#Print the number of missing values for each feature\n",
    "numeric_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on that function, There are NaN values for the majority of DataSource3_Feature1 & DataSource3_Feature2,\n",
    "#so we will simply remove those features and see where we stand in terms of remaining NaN values\n",
    "\n",
    "numeric_dataset.drop('DataSource3_Feature1', axis = 1, inplace = True)\n",
    "numeric_dataset.drop('DataSource3_Feature2', axis = 1, inplace = True)\n",
    "null_data = numeric_dataset[numeric_dataset.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea8026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a lot better when considering missing values, as that's a reasonable threshold to simply remove\n",
    "#anything containing NaN values without sacrificing the statistical significance of our sample too harshly, but we\n",
    "#can likely do better\n",
    "numeric_dataset.isna().sum(axis = 1).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at that function call, it looks like theres a handful of rows with a high proportion of null values,\n",
    "#so we'll remove those manually and reassess.\n",
    "#Of the multiple values I tried, 7 appeared to be a sweet spot for a rejection threshold.\n",
    "\n",
    "clean_data = numeric_dataset[numeric_dataset.isnull().sum(axis=1) < 7]\n",
    "clean_data[clean_data.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fed378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the remaining missing values, we'll use a Nearest Neighbors implementation to fill them out,\n",
    "#This requires us to normalize our data first, which we were going to do anyway. \n",
    "\n",
    "#Normalize the data using the Min-Max Method\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "clean_data = pd.DataFrame(scaler.fit_transform(clean_data),\n",
    "                          columns = clean_data.columns)\n",
    "\n",
    "#Use KNN on the scaled data to fill  in the missing values\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "clean_data = pd.DataFrame(imputer.fit_transform(clean_data),\n",
    "                          columns = clean_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54870b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataSource2_Feature2 and DataSource2_Feature3 appear to be identical, so We'll verify that\n",
    "clean_data.query('DataSource2_Feature2 != DataSource2_Feature3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#And, since they are, in fact, identical, we'll remove one of them\n",
    "clean_data.drop('DataSource2_Feature3', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#While We're at it, We'll pop out the target column\n",
    "target = clean_data.pop('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f142e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the pandas objects to numpy arrays\n",
    "X = clean_data.to_numpy()\n",
    "Y = target.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3592bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the Mutual Information algorithm to reduce dimensionality, determining the most useful features \n",
    "importances = mutual_info_classif(X,Y)\n",
    "feat_importances = pd.Series(importances, clean_data.columns[0:len(clean_data.columns)])\n",
    "feat_importances.plot(kind='barh',color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminate the least useful features until the most useful 13 (A sweet-spot found through experimentation) remain \n",
    "while(len(clean_data.columns)>13):\n",
    "    clean_data.pop(clean_data.columns[np.argmin(importances)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-Create the training set with the modified features\n",
    "X = clean_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up a K-Folds Cross-Validation, with k=5\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(26,input_dim=13,activation='relu'))\n",
    "    model.add(Dense(52,input_dim=26,activation='relu'))\n",
    "    model.add(Dense(104,input_dim=52,activation='relu'))\n",
    "    model.add(Dense(52,input_dim=104,activation='relu'))\n",
    "    model.add(Dense(26,input_dim=52,activation='relu'))\n",
    "    model.add(Dense(13,input_dim=26,activation='relu'))\n",
    "    model.add(Dense(1, input_dim=13, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['AUC'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(X[train],\n",
    "                        Y[train],\n",
    "                        epochs=750,\n",
    "                        batch_size=100,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=[callback])\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=1)\n",
    "    \n",
    "    #Plot the Training / Validation Loss\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa5249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a new confusion matrix\n",
    "y_pred = model.predict(X[test])\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "cm = confusion_matrix(y_pred, Y[test])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Neural Network appears to only be predicting 0, which is not entirely suprising that one of the models would end up doing\n",
    "#that Given the imbalance of the data (again, these models were additionally tuned with methods for helping with imbalanced\n",
    "#data, such as class weighting, SMOTE, etc. to no avail)\n",
    "\n",
    "#My hypothesis is then that the positive classifications are not unique enough in their features to effectively stand out\n",
    "#in the data. \n",
    "\n",
    "#As such, I'll save the second logistic regression, as it had a slight performance gain over the other models and will be \n",
    "#one of the more efficent options in terms of storage / speed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
